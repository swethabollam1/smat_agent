{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "kernelaee2b70308 (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijTi4kGNxCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAPiA3qiNxCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fab9eddb-0b48-4694-b5b0-cd7beb5ea126"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from numpy import argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from numpy import array_equal\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiWLBV_KNxCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "import pandas as pd \n",
        "from pandas.io.json import json_normalize #package for flattening json in pandas df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXvLWd-OGF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUrm-zwUOMET",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "aae0f5e3-85d4-4b14-af92-f50a49f17942"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-77d71075-d57d-4ec8-ab82-33ba23f53547\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-77d71075-d57d-4ec8-ab82-33ba23f53547\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving text.csv to text.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTWwcV-ENxC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eb922839-4dea-4b5f-cd01-3939be022f08"
      },
      "source": [
        "text=pd.read_csv('text.csv')\n",
        "text.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'd like to book a trip to Atlantis from Capri...</td>\n",
              "      <td>Hi...I checked a few options for you, and unfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yes, how about going to Neverland from Caprica...</td>\n",
              "      <td>I checked the availability for this date and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I have no flexibility for dates... but I can l...</td>\n",
              "      <td>I checked the availability for that date and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello, I am looking to book a vacation from Go...</td>\n",
              "      <td>Hi. Sorry, I can't find any trips from Gotham ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What about a trip from Gotham City to Neverlan...</td>\n",
              "      <td>Sorry, I cannot find any trips leaving from Go...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message                                           response\n",
              "0  I'd like to book a trip to Atlantis from Capri...  Hi...I checked a few options for you, and unfo...\n",
              "1  Yes, how about going to Neverland from Caprica...  I checked the availability for this date and t...\n",
              "2  I have no flexibility for dates... but I can l...  I checked the availability for that date and t...\n",
              "3  Hello, I am looking to book a vacation from Go...  Hi. Sorry, I can't find any trips from Gotham ...\n",
              "4  What about a trip from Gotham City to Neverlan...  Sorry, I cannot find any trips leaving from Go..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A4EKR5hPdXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ea538028-8a2e-4a24-a71f-eb5fe65b1857"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ggaFKG-NxC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99b61a71-1734-4eb5-931d-a5b33da0326f"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "from tqdm import tqdm\n",
        "preprocessed_que = []\n",
        "preprocessed_ans = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in tqdm(text['message'].values):\n",
        "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
        "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
        "    sentance = decontracted(sentance)\n",
        "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
        "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
        "    preprocessed_que.append(sentance.strip())\n",
        "\n",
        "for sentance in tqdm(text['response'].values):\n",
        "    sentance = re.sub(r\"http\\S+\", \"\", str(sentance))\n",
        "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
        "    sentance = decontracted(sentance)\n",
        "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
        "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
        "    preprocessed_ans.append(sentance.strip())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9579/9579 [00:02<00:00, 4496.27it/s]\n",
            "100%|██████████| 9579/9579 [00:02<00:00, 4295.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBhF9O3NxC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c0c199a-5a42-4e3c-e581-1b43f5e030d6"
      },
      "source": [
        "text['message'] = preprocessed_que\n",
        "text['response'] = preprocessed_ans\n",
        "text.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I would like to book a trip to Atlantis from C...</td>\n",
              "      <td>Hi I checked a few options for you and unfortu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yes how about going to Neverland from Caprica ...</td>\n",
              "      <td>I checked the availability for this date and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I have no flexibility for dates but I can leav...</td>\n",
              "      <td>I checked the availability for that date and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello I am looking to book a vacation from Got...</td>\n",
              "      <td>Hi Sorry I can not find any trips from Gotham ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What about a trip from Gotham City to Neverlan...</td>\n",
              "      <td>Sorry I cannot find any trips leaving from Got...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message                                           response\n",
              "0  I would like to book a trip to Atlantis from C...  Hi I checked a few options for you and unfortu...\n",
              "1  Yes how about going to Neverland from Caprica ...  I checked the availability for this date and t...\n",
              "2  I have no flexibility for dates but I can leav...  I checked the availability for that date and t...\n",
              "3  Hello I am looking to book a vacation from Got...  Hi Sorry I can not find any trips from Gotham ...\n",
              "4  What about a trip from Gotham City to Neverlan...  Sorry I cannot find any trips leaving from Got..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVMdpgbkCOUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=text[0:9000]\n",
        "test=text[9000: ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoIrAudkNxC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input=train['message'].values\n",
        "test_input=test['message'].values\n",
        "\n",
        "train_target=train['response'].values\n",
        "test_target=test['response'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbAy3DqFNxDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_input_len = 60\n",
        "\n",
        "embed_size = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=500)\n",
        "tokenizer.fit_on_texts(list(train_input))\n",
        "\n",
        "train_q = tokenizer.texts_to_sequences(train_input)\n",
        "test_q = tokenizer.texts_to_sequences(test_input)\n",
        "\n",
        "x_train = pad_sequences(train_q, maxlen=max_input_len)\n",
        "x_test = pad_sequences(test_q, maxlen=max_input_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMgSvnR8NxDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_output_len=60\n",
        "t = Tokenizer(num_words=500)\n",
        "t.fit_on_texts(list(train_target))\n",
        "\n",
        "train_a = t.texts_to_sequences(train_target)\n",
        "test_a = t.texts_to_sequences(test_target)\n",
        "\n",
        "y_train = pad_sequences(train_a,maxlen=max_output_len)\n",
        "y_test = pad_sequences(test_a,maxlen=max_output_len)\n",
        "\n",
        "    \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "    return [argmax(vector) for vector in encoded_seq]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qxwpMLpNxDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1, X2, y = list(), list(), list()\n",
        "\n",
        "target_in = y_train\n",
        "    \n",
        "src_encoded = to_categorical(x_train)\n",
        "tar_encoded = to_categorical(y_train)\n",
        "tar2_encoded = to_categorical(target_in)\n",
        "\n",
        "X1.append(src_encoded)\n",
        "X2.append(tar2_encoded)\n",
        "y.append(tar_encoded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRLDAf6jCBjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eacc8841-1d52-4871-b7ec-5bcc10adeeaf"
      },
      "source": [
        "X1=array(X1).reshape(9000,60,500)\n",
        "X2=array(X2).reshape(9000,60,500)\n",
        "y=array(y).reshape(9000,60,500)\n",
        "\n",
        "print(X1.shape, X2.shape, y.shape)\n",
        "print('X1=%s, X2=%s, y=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9000, 60, 500) (9000, 60, 500) (9000, 60, 500)\n",
            "X1=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 19, 29, 2, 26, 5, 41, 2, 11, 27, 438, 44, 8, 74, 1, 15, 5, 496, 51, 18], X2=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 180, 7, 2, 268, 142, 8, 1, 5, 99, 32, 23, 50, 9, 62, 105, 31, 20, 11, 1, 26, 4, 51, 75, 431, 93, 165], y=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 180, 7, 2, 268, 142, 8, 1, 5, 99, 32, 23, 50, 9, 62, 105, 31, 20, 11, 1, 26, 4, 51, 75, 431, 93, 165]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrWG4DRRNxDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1b5677af-69f5-4255-c688-3197e4c3ad46"
      },
      "source": [
        "def define_models(n_input, n_output, n_units):\n",
        "# define training encoder\n",
        "    encoder_inputs = Input(shape=(None, n_input))\n",
        "    encoder = LSTM(n_units, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "# define training decoder\n",
        "    decoder_inputs = Input(shape=(None, n_output))\n",
        "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(n_output, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# define inference encoder\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# define inference decoder\n",
        "    decoder_state_input_h = Input(shape=(n_units,))\n",
        "    decoder_state_input_c = Input(shape=(n_units,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "# return all models\n",
        "    return model, encoder_model, decoder_model\n",
        " \n",
        "# generate target given source sequence\n",
        "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
        "    # encode\n",
        "    state = infenc.predict(source)\n",
        "    # start of sequence input\n",
        "    target_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
        "    # collect predictions\n",
        "    output = list()\n",
        "    for t in range(n_steps):\n",
        "    # predict next char\n",
        "        yhat, h, c = infdec.predict([target_seq] + state)\n",
        "    # store prediction\n",
        "        output.append(yhat[0,0,:])\n",
        "    # update state\n",
        "        state = [h, c]\n",
        "    # update target sequence\n",
        "        target_seq = yhat\n",
        "    return array(output)\n",
        " \n",
        "# configure problem\n",
        "n_features = 500\n",
        "n_steps_in = 60\n",
        "n_steps_out = 60\n",
        "# define model\n",
        "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
        "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "train.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, None, 500)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           (None, None, 500)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, 128), (None, 322048      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, None, 128),  322048      input_10[0][0]                   \n",
            "                                                                 lstm_5[0][1]                     \n",
            "                                                                 lstm_5[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, None, 500)    64500       lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 708,596\n",
            "Trainable params: 708,596\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2_WsxsNxDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4255388-fdc6-466d-97b6-2b0d490cacce"
      },
      "source": [
        "history=train.fit([X1, X2], y, epochs=200,validation_split=0.2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7200 samples, validate on 1800 samples\n",
            "Epoch 1/200\n",
            "7200/7200 [==============================] - 39s 5ms/step - loss: 1.7670 - acc: 0.7725 - val_loss: 1.2908 - val_acc: 0.7803\n",
            "Epoch 2/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.1303 - acc: 0.7989 - val_loss: 1.0007 - val_acc: 0.8315\n",
            "Epoch 3/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.8228 - acc: 0.8707 - val_loss: 0.6698 - val_acc: 0.9089\n",
            "Epoch 4/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.5209 - acc: 0.9273 - val_loss: 0.4304 - val_acc: 0.9398\n",
            "Epoch 5/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.3395 - acc: 0.9539 - val_loss: 0.3012 - val_acc: 0.9598\n",
            "Epoch 6/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.2368 - acc: 0.9699 - val_loss: 0.2210 - val_acc: 0.9716\n",
            "Epoch 7/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.1715 - acc: 0.9782 - val_loss: 0.1673 - val_acc: 0.9775\n",
            "Epoch 8/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.1283 - acc: 0.9831 - val_loss: 0.1307 - val_acc: 0.9813\n",
            "Epoch 9/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0981 - acc: 0.9871 - val_loss: 0.1041 - val_acc: 0.9850\n",
            "Epoch 10/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0757 - acc: 0.9900 - val_loss: 0.0830 - val_acc: 0.9883\n",
            "Epoch 11/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0580 - acc: 0.9931 - val_loss: 0.0657 - val_acc: 0.9916\n",
            "Epoch 12/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0437 - acc: 0.9956 - val_loss: 0.0508 - val_acc: 0.9948\n",
            "Epoch 13/200\n",
            "7200/7200 [==============================] - 36s 5ms/step - loss: 0.0322 - acc: 0.9974 - val_loss: 0.0384 - val_acc: 0.9965\n",
            "Epoch 14/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0232 - acc: 0.9984 - val_loss: 0.0284 - val_acc: 0.9980\n",
            "Epoch 15/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0164 - acc: 0.9991 - val_loss: 0.0206 - val_acc: 0.9987\n",
            "Epoch 16/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0116 - acc: 0.9995 - val_loss: 0.0146 - val_acc: 0.9994\n",
            "Epoch 17/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0082 - acc: 0.9997 - val_loss: 0.0106 - val_acc: 0.9995\n",
            "Epoch 18/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.0059 - acc: 0.9998 - val_loss: 0.0078 - val_acc: 0.9996\n",
            "Epoch 19/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.0043 - acc: 0.9998 - val_loss: 0.0058 - val_acc: 0.9997\n",
            "Epoch 20/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.0033 - acc: 0.9999 - val_loss: 0.0046 - val_acc: 0.9997\n",
            "Epoch 21/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.0036 - val_acc: 0.9998\n",
            "Epoch 22/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9999\n",
            "Epoch 23/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
            "Epoch 24/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9999\n",
            "Epoch 25/200\n",
            "7200/7200 [==============================] - 39s 5ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9999\n",
            "Epoch 26/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 8.8480e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9999\n",
            "Epoch 27/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 7.3998e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9999\n",
            "Epoch 28/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 6.2736e-04 - acc: 1.0000 - val_loss: 9.9551e-04 - val_acc: 0.9999\n",
            "Epoch 29/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.4003e-04 - acc: 1.0000 - val_loss: 8.4380e-04 - val_acc: 1.0000\n",
            "Epoch 30/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.6953e-04 - acc: 1.0000 - val_loss: 7.6593e-04 - val_acc: 0.9999\n",
            "Epoch 31/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.9367e-04 - acc: 1.0000 - val_loss: 6.4692e-04 - val_acc: 1.0000\n",
            "Epoch 32/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.4021e-04 - acc: 1.0000 - val_loss: 5.6508e-04 - val_acc: 1.0000\n",
            "Epoch 33/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.9808e-04 - acc: 1.0000 - val_loss: 5.0738e-04 - val_acc: 1.0000\n",
            "Epoch 34/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.5662e-04 - acc: 1.0000 - val_loss: 4.8008e-04 - val_acc: 1.0000\n",
            "Epoch 35/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.2410e-04 - acc: 1.0000 - val_loss: 3.9182e-04 - val_acc: 1.0000\n",
            "Epoch 36/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.9675e-04 - acc: 1.0000 - val_loss: 3.9184e-04 - val_acc: 1.0000\n",
            "Epoch 37/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.7323e-04 - acc: 1.0000 - val_loss: 3.6261e-04 - val_acc: 1.0000\n",
            "Epoch 38/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.5070e-04 - acc: 1.0000 - val_loss: 2.9991e-04 - val_acc: 1.0000\n",
            "Epoch 39/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.3462e-04 - acc: 1.0000 - val_loss: 2.5418e-04 - val_acc: 1.0000\n",
            "Epoch 40/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.1816e-04 - acc: 1.0000 - val_loss: 2.5153e-04 - val_acc: 1.0000\n",
            "Epoch 41/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.0397e-04 - acc: 1.0000 - val_loss: 2.4361e-04 - val_acc: 1.0000\n",
            "Epoch 42/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 9.3519e-05 - acc: 1.0000 - val_loss: 2.0327e-04 - val_acc: 1.0000\n",
            "Epoch 43/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 8.1840e-05 - acc: 1.0000 - val_loss: 2.0250e-04 - val_acc: 1.0000\n",
            "Epoch 44/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 7.2713e-05 - acc: 1.0000 - val_loss: 1.5029e-04 - val_acc: 1.0000\n",
            "Epoch 45/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.5080e-05 - acc: 1.0000 - val_loss: 1.2971e-04 - val_acc: 1.0000\n",
            "Epoch 46/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.7443e-05 - acc: 1.0000 - val_loss: 1.4668e-04 - val_acc: 1.0000\n",
            "Epoch 47/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.1747e-05 - acc: 1.0000 - val_loss: 1.3271e-04 - val_acc: 1.0000\n",
            "Epoch 48/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.5880e-05 - acc: 1.0000 - val_loss: 1.1887e-04 - val_acc: 1.0000\n",
            "Epoch 49/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.1145e-05 - acc: 1.0000 - val_loss: 1.1265e-04 - val_acc: 1.0000\n",
            "Epoch 50/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.6568e-05 - acc: 1.0000 - val_loss: 8.7755e-05 - val_acc: 1.0000\n",
            "Epoch 51/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.3417e-05 - acc: 1.0000 - val_loss: 8.4527e-05 - val_acc: 1.0000\n",
            "Epoch 52/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9595e-05 - acc: 1.0000 - val_loss: 1.0303e-04 - val_acc: 1.0000\n",
            "Epoch 53/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.6631e-05 - acc: 1.0000 - val_loss: 8.1695e-05 - val_acc: 1.0000\n",
            "Epoch 54/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.3862e-05 - acc: 1.0000 - val_loss: 6.5286e-05 - val_acc: 1.0000\n",
            "Epoch 55/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.1456e-05 - acc: 1.0000 - val_loss: 6.1284e-05 - val_acc: 1.0000\n",
            "Epoch 56/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.9361e-05 - acc: 1.0000 - val_loss: 5.8012e-05 - val_acc: 1.0000\n",
            "Epoch 57/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.7708e-05 - acc: 1.0000 - val_loss: 6.5978e-05 - val_acc: 1.0000\n",
            "Epoch 58/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.5833e-05 - acc: 1.0000 - val_loss: 4.6549e-05 - val_acc: 1.0000\n",
            "Epoch 59/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.4320e-05 - acc: 1.0000 - val_loss: 6.3636e-05 - val_acc: 1.0000\n",
            "Epoch 60/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.3028e-05 - acc: 1.0000 - val_loss: 4.5710e-05 - val_acc: 1.0000\n",
            "Epoch 61/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.1828e-05 - acc: 1.0000 - val_loss: 2.8253e-05 - val_acc: 1.0000\n",
            "Epoch 62/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.0705e-05 - acc: 1.0000 - val_loss: 3.0211e-05 - val_acc: 1.0000\n",
            "Epoch 63/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 9.6382e-06 - acc: 1.0000 - val_loss: 3.8223e-05 - val_acc: 1.0000\n",
            "Epoch 64/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 8.7736e-06 - acc: 1.0000 - val_loss: 4.1467e-05 - val_acc: 1.0000\n",
            "Epoch 65/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 8.0040e-06 - acc: 1.0000 - val_loss: 2.6294e-05 - val_acc: 1.0000\n",
            "Epoch 66/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 7.2986e-06 - acc: 1.0000 - val_loss: 2.3819e-05 - val_acc: 1.0000\n",
            "Epoch 67/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 6.6503e-06 - acc: 1.0000 - val_loss: 1.9146e-05 - val_acc: 1.0000\n",
            "Epoch 68/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 6.1252e-06 - acc: 1.0000 - val_loss: 2.2267e-05 - val_acc: 1.0000\n",
            "Epoch 69/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 5.6227e-06 - acc: 1.0000 - val_loss: 2.1369e-05 - val_acc: 1.0000\n",
            "Epoch 70/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.1155e-06 - acc: 1.0000 - val_loss: 2.4265e-05 - val_acc: 1.0000\n",
            "Epoch 71/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.7084e-06 - acc: 1.0000 - val_loss: 1.7089e-05 - val_acc: 1.0000\n",
            "Epoch 72/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.3132e-06 - acc: 1.0000 - val_loss: 1.9879e-05 - val_acc: 1.0000\n",
            "Epoch 73/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.9897e-06 - acc: 1.0000 - val_loss: 1.6983e-05 - val_acc: 1.0000\n",
            "Epoch 74/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.6859e-06 - acc: 1.0000 - val_loss: 1.7585e-05 - val_acc: 1.0000\n",
            "Epoch 75/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.4113e-06 - acc: 1.0000 - val_loss: 9.3219e-06 - val_acc: 1.0000\n",
            "Epoch 76/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.1648e-06 - acc: 1.0000 - val_loss: 1.2877e-05 - val_acc: 1.0000\n",
            "Epoch 77/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9508e-06 - acc: 1.0000 - val_loss: 1.1503e-05 - val_acc: 1.0000\n",
            "Epoch 78/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.7303e-06 - acc: 1.0000 - val_loss: 9.3749e-06 - val_acc: 1.0000\n",
            "Epoch 79/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.5448e-06 - acc: 1.0000 - val_loss: 9.1329e-06 - val_acc: 1.0000\n",
            "Epoch 80/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.3838e-06 - acc: 1.0000 - val_loss: 9.2598e-06 - val_acc: 1.0000\n",
            "Epoch 81/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.2258e-06 - acc: 1.0000 - val_loss: 7.3122e-06 - val_acc: 1.0000\n",
            "Epoch 82/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.0890e-06 - acc: 1.0000 - val_loss: 9.5261e-06 - val_acc: 1.0000\n",
            "Epoch 83/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.9616e-06 - acc: 1.0000 - val_loss: 7.1870e-06 - val_acc: 1.0000\n",
            "Epoch 84/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.8722e-06 - acc: 1.0000 - val_loss: 7.0551e-06 - val_acc: 1.0000\n",
            "Epoch 85/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.7448e-06 - acc: 1.0000 - val_loss: 7.6127e-06 - val_acc: 1.0000\n",
            "Epoch 86/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.6408e-06 - acc: 1.0000 - val_loss: 7.1676e-06 - val_acc: 1.0000\n",
            "Epoch 87/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.5608e-06 - acc: 1.0000 - val_loss: 5.7809e-06 - val_acc: 1.0000\n",
            "Epoch 88/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.4748e-06 - acc: 1.0000 - val_loss: 4.9237e-06 - val_acc: 1.0000\n",
            "Epoch 89/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.3993e-06 - acc: 1.0000 - val_loss: 5.3277e-06 - val_acc: 1.0000\n",
            "Epoch 90/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.3329e-06 - acc: 1.0000 - val_loss: 4.5547e-06 - val_acc: 1.0000\n",
            "Epoch 91/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.2777e-06 - acc: 1.0000 - val_loss: 3.5168e-06 - val_acc: 1.0000\n",
            "Epoch 92/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 1.2160e-06 - acc: 1.0000 - val_loss: 3.6378e-06 - val_acc: 1.0000\n",
            "Epoch 93/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.1616e-06 - acc: 1.0000 - val_loss: 3.7616e-06 - val_acc: 1.0000\n",
            "Epoch 94/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.1138e-06 - acc: 1.0000 - val_loss: 4.0935e-06 - val_acc: 1.0000\n",
            "Epoch 95/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.0676e-06 - acc: 1.0000 - val_loss: 4.4318e-06 - val_acc: 1.0000\n",
            "Epoch 96/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 1.0275e-06 - acc: 1.0000 - val_loss: 3.0401e-06 - val_acc: 1.0000\n",
            "Epoch 97/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 9.8489e-07 - acc: 1.0000 - val_loss: 3.1727e-06 - val_acc: 1.0000\n",
            "Epoch 98/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 9.6026e-07 - acc: 1.0000 - val_loss: 2.9886e-06 - val_acc: 1.0000\n",
            "Epoch 99/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 9.1712e-07 - acc: 1.0000 - val_loss: 2.7446e-06 - val_acc: 1.0000\n",
            "Epoch 100/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 8.8626e-07 - acc: 1.0000 - val_loss: 2.7965e-06 - val_acc: 1.0000\n",
            "Epoch 101/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 8.5438e-07 - acc: 1.0000 - val_loss: 2.4920e-06 - val_acc: 1.0000\n",
            "Epoch 102/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 8.3170e-07 - acc: 1.0000 - val_loss: 2.0486e-06 - val_acc: 1.0000\n",
            "Epoch 103/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 8.0294e-07 - acc: 1.0000 - val_loss: 2.0674e-06 - val_acc: 1.0000\n",
            "Epoch 104/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 7.7947e-07 - acc: 1.0000 - val_loss: 1.7981e-06 - val_acc: 1.0000\n",
            "Epoch 105/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 7.5844e-07 - acc: 1.0000 - val_loss: 1.8888e-06 - val_acc: 1.0000\n",
            "Epoch 106/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 7.3523e-07 - acc: 1.0000 - val_loss: 1.9167e-06 - val_acc: 1.0000\n",
            "Epoch 107/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 7.1897e-07 - acc: 1.0000 - val_loss: 1.7021e-06 - val_acc: 1.0000\n",
            "Epoch 108/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 6.9855e-07 - acc: 1.0000 - val_loss: 1.6169e-06 - val_acc: 1.0000\n",
            "Epoch 109/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.8131e-07 - acc: 1.0000 - val_loss: 1.6683e-06 - val_acc: 1.0000\n",
            "Epoch 110/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.6234e-07 - acc: 1.0000 - val_loss: 1.6195e-06 - val_acc: 1.0000\n",
            "Epoch 111/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.4712e-07 - acc: 1.0000 - val_loss: 1.5190e-06 - val_acc: 1.0000\n",
            "Epoch 112/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.3349e-07 - acc: 1.0000 - val_loss: 1.4018e-06 - val_acc: 1.0000\n",
            "Epoch 113/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 6.1912e-07 - acc: 1.0000 - val_loss: 1.5190e-06 - val_acc: 1.0000\n",
            "Epoch 114/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 6.0533e-07 - acc: 1.0000 - val_loss: 1.3340e-06 - val_acc: 1.0000\n",
            "Epoch 115/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.9183e-07 - acc: 1.0000 - val_loss: 1.3543e-06 - val_acc: 1.0000\n",
            "Epoch 116/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.7981e-07 - acc: 1.0000 - val_loss: 1.3159e-06 - val_acc: 1.0000\n",
            "Epoch 117/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 5.7043e-07 - acc: 1.0000 - val_loss: 1.2208e-06 - val_acc: 1.0000\n",
            "Epoch 118/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.5833e-07 - acc: 1.0000 - val_loss: 1.1901e-06 - val_acc: 1.0000\n",
            "Epoch 119/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.4801e-07 - acc: 1.0000 - val_loss: 1.1942e-06 - val_acc: 1.0000\n",
            "Epoch 120/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.3726e-07 - acc: 1.0000 - val_loss: 1.1797e-06 - val_acc: 1.0000\n",
            "Epoch 121/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 5.2747e-07 - acc: 1.0000 - val_loss: 1.1489e-06 - val_acc: 1.0000\n",
            "Epoch 122/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 5.1993e-07 - acc: 1.0000 - val_loss: 1.0995e-06 - val_acc: 1.0000\n",
            "Epoch 123/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 5.1000e-07 - acc: 1.0000 - val_loss: 1.1054e-06 - val_acc: 1.0000\n",
            "Epoch 124/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 5.0171e-07 - acc: 1.0000 - val_loss: 1.0411e-06 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.9371e-07 - acc: 1.0000 - val_loss: 1.0384e-06 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.8597e-07 - acc: 1.0000 - val_loss: 1.0315e-06 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.7887e-07 - acc: 1.0000 - val_loss: 1.0221e-06 - val_acc: 1.0000\n",
            "Epoch 128/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 4.7195e-07 - acc: 1.0000 - val_loss: 1.0030e-06 - val_acc: 1.0000\n",
            "Epoch 129/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.6500e-07 - acc: 1.0000 - val_loss: 1.0017e-06 - val_acc: 1.0000\n",
            "Epoch 130/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.5827e-07 - acc: 1.0000 - val_loss: 9.5100e-07 - val_acc: 1.0000\n",
            "Epoch 131/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.5246e-07 - acc: 1.0000 - val_loss: 9.7142e-07 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.4652e-07 - acc: 1.0000 - val_loss: 9.3627e-07 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "7200/7200 [==============================] - 39s 5ms/step - loss: 4.4047e-07 - acc: 1.0000 - val_loss: 9.3712e-07 - val_acc: 1.0000\n",
            "Epoch 134/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.3478e-07 - acc: 1.0000 - val_loss: 9.1742e-07 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.2912e-07 - acc: 1.0000 - val_loss: 9.1010e-07 - val_acc: 1.0000\n",
            "Epoch 136/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.2386e-07 - acc: 1.0000 - val_loss: 8.7697e-07 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.1912e-07 - acc: 1.0000 - val_loss: 8.5383e-07 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.1439e-07 - acc: 1.0000 - val_loss: 8.4046e-07 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.0923e-07 - acc: 1.0000 - val_loss: 8.4965e-07 - val_acc: 1.0000\n",
            "Epoch 140/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.0454e-07 - acc: 1.0000 - val_loss: 8.2033e-07 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 4.0010e-07 - acc: 1.0000 - val_loss: 8.2379e-07 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.9587e-07 - acc: 1.0000 - val_loss: 8.5387e-07 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.9188e-07 - acc: 1.0000 - val_loss: 8.0424e-07 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.8760e-07 - acc: 1.0000 - val_loss: 7.8154e-07 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.8335e-07 - acc: 1.0000 - val_loss: 7.7708e-07 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.7938e-07 - acc: 1.0000 - val_loss: 7.6433e-07 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.7574e-07 - acc: 1.0000 - val_loss: 7.7564e-07 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.7215e-07 - acc: 1.0000 - val_loss: 7.5285e-07 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "7200/7200 [==============================] - 39s 5ms/step - loss: 3.6855e-07 - acc: 1.0000 - val_loss: 7.6770e-07 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.6496e-07 - acc: 1.0000 - val_loss: 7.5483e-07 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.6165e-07 - acc: 1.0000 - val_loss: 7.2812e-07 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.5863e-07 - acc: 1.0000 - val_loss: 7.1961e-07 - val_acc: 1.0000\n",
            "Epoch 153/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.5516e-07 - acc: 1.0000 - val_loss: 7.1398e-07 - val_acc: 1.0000\n",
            "Epoch 154/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.5229e-07 - acc: 1.0000 - val_loss: 7.0422e-07 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.4914e-07 - acc: 1.0000 - val_loss: 7.0298e-07 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.4649e-07 - acc: 1.0000 - val_loss: 6.8421e-07 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.4360e-07 - acc: 1.0000 - val_loss: 6.7935e-07 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.4059e-07 - acc: 1.0000 - val_loss: 6.7754e-07 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.3768e-07 - acc: 1.0000 - val_loss: 6.6695e-07 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.3513e-07 - acc: 1.0000 - val_loss: 6.6889e-07 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.3230e-07 - acc: 1.0000 - val_loss: 6.5406e-07 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.2965e-07 - acc: 1.0000 - val_loss: 6.4818e-07 - val_acc: 1.0000\n",
            "Epoch 163/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.2711e-07 - acc: 1.0000 - val_loss: 6.4872e-07 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.2489e-07 - acc: 1.0000 - val_loss: 6.4817e-07 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.2231e-07 - acc: 1.0000 - val_loss: 6.5834e-07 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.2016e-07 - acc: 1.0000 - val_loss: 6.2775e-07 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.1775e-07 - acc: 1.0000 - val_loss: 6.1592e-07 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.1546e-07 - acc: 1.0000 - val_loss: 6.1423e-07 - val_acc: 1.0000\n",
            "Epoch 169/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.1337e-07 - acc: 1.0000 - val_loss: 6.1016e-07 - val_acc: 1.0000\n",
            "Epoch 170/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 3.1115e-07 - acc: 1.0000 - val_loss: 6.1819e-07 - val_acc: 1.0000\n",
            "Epoch 171/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.0895e-07 - acc: 1.0000 - val_loss: 5.9448e-07 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.0677e-07 - acc: 1.0000 - val_loss: 5.9015e-07 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.0482e-07 - acc: 1.0000 - val_loss: 5.8729e-07 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.0296e-07 - acc: 1.0000 - val_loss: 5.8543e-07 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 3.0113e-07 - acc: 1.0000 - val_loss: 5.7609e-07 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9893e-07 - acc: 1.0000 - val_loss: 5.7956e-07 - val_acc: 1.0000\n",
            "Epoch 177/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9726e-07 - acc: 1.0000 - val_loss: 5.7849e-07 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9530e-07 - acc: 1.0000 - val_loss: 5.7582e-07 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.9366e-07 - acc: 1.0000 - val_loss: 5.6791e-07 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.9198e-07 - acc: 1.0000 - val_loss: 5.6203e-07 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.9010e-07 - acc: 1.0000 - val_loss: 5.6385e-07 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.8833e-07 - acc: 1.0000 - val_loss: 5.6910e-07 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.8655e-07 - acc: 1.0000 - val_loss: 5.4131e-07 - val_acc: 1.0000\n",
            "Epoch 184/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.8500e-07 - acc: 1.0000 - val_loss: 5.4887e-07 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.8340e-07 - acc: 1.0000 - val_loss: 5.4559e-07 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.8186e-07 - acc: 1.0000 - val_loss: 5.4949e-07 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.8032e-07 - acc: 1.0000 - val_loss: 5.3304e-07 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.7873e-07 - acc: 1.0000 - val_loss: 5.3492e-07 - val_acc: 1.0000\n",
            "Epoch 189/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.7722e-07 - acc: 1.0000 - val_loss: 5.4038e-07 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.7574e-07 - acc: 1.0000 - val_loss: 5.2575e-07 - val_acc: 1.0000\n",
            "Epoch 191/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.7430e-07 - acc: 1.0000 - val_loss: 5.2443e-07 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.7292e-07 - acc: 1.0000 - val_loss: 5.2165e-07 - val_acc: 1.0000\n",
            "Epoch 193/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.7150e-07 - acc: 1.0000 - val_loss: 5.1063e-07 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.7012e-07 - acc: 1.0000 - val_loss: 5.1297e-07 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.6872e-07 - acc: 1.0000 - val_loss: 5.0771e-07 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.6748e-07 - acc: 1.0000 - val_loss: 5.2016e-07 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.6616e-07 - acc: 1.0000 - val_loss: 5.1062e-07 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "7200/7200 [==============================] - 37s 5ms/step - loss: 2.6476e-07 - acc: 1.0000 - val_loss: 5.0308e-07 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.6353e-07 - acc: 1.0000 - val_loss: 5.0509e-07 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "7200/7200 [==============================] - 38s 5ms/step - loss: 2.6239e-07 - acc: 1.0000 - val_loss: 4.9631e-07 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GJBNVSQrpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "00ba580d-4da2-4fae-f187-5d679c8061d2"
      },
      "source": [
        "train.save(\"lstm_model.h5\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_5/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_5/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL6czNys4SE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.save_weights(\"lstm_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmPMYtf0yXm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_1, X_2, Y = list(), list(), list()\n",
        "\n",
        "target_in = y_test\n",
        "    \n",
        "src_encoded = to_categorical(x_test,num_classes=500)\n",
        "tar_encoded = to_categorical(y_test)\n",
        "tar2_encoded = to_categorical(target_in)\n",
        "\n",
        "X_1.append(src_encoded)\n",
        "X_2.append(tar2_encoded)\n",
        "Y.append(tar_encoded)\n",
        "\n",
        "X_1=array(X_1).reshape(579,60,500)\n",
        "X_2=array(X_2).reshape(579,60,500)\n",
        "Y=array(Y).reshape(579,60,500)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QNDrNGnBE8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1=X_1[0:1:,]\n",
        "x2=X_1[1:2:,]\n",
        "x3=X_1[2:3:,]\n",
        "x4=X_1[3:4:,]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nkadDIaA8-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6f314deb-097f-4a17-bbcb-5a882919b61a"
      },
      "source": [
        "target = predict_sequence(infenc, infdec, x2, n_steps_out, n_features)\n",
        "print('X=%s y=%s, yhat=%s' % (one_hot_decode(X_1[1]), one_hot_decode(Y[1]),one_hot_decode(target)))\n",
        "y_pred=one_hot_decode(target)\n",
        "a=t.word_index\n",
        "b=list(a.keys())\n",
        "reply=[]\n",
        "for i in range(2115):\n",
        "    for j in range(60):\n",
        "        if y_pred[j]==i+1:\n",
        "            reply.append(b[i])\n",
        "\n",
        "print(\"*****************EMAIL_BODY********************\\n\")\n",
        "\n",
        "print(test_input[1])\n",
        "\n",
        "print(\"*******************Suggested Reply**************\")\n",
        "\n",
        "print(' '.join(reply))\n",
        "\n",
        "print(\"*******************Actual Reply******************\")\n",
        "print(test_target[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 25, 55, 87, 151, 155, 14, 428] y=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 3, 57, 2, 14, 38, 6, 40, 127, 21, 61, 5, 71, 21, 44, 3, 19, 87, 16, 34, 21, 30, 54, 8, 78, 16, 1], yhat=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "*****************EMAIL_BODY********************\n",
            "\n",
            "Is there anything better than stars in Cordoba\n",
            "*******************Suggested Reply**************\n",
            "\n",
            "*******************Actual Reply******************\n",
            "In Cordoba the Everland has a star rating is available within your dates and also sits your budget The package price of will cover your day stay for all of you\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}