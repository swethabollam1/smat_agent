{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import json \n",
    "import pandas as pd \n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('frames.json') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "#lets put the data into a pandas df\n",
    "#clicking on raw_nyc_phil.json under \"Input Files\"\n",
    "#tells us parent node is 'programs'\n",
    "text=[]\n",
    "for i in range(len(d)):\n",
    "    nycphil = json_normalize(d[i]['turns'])\n",
    "    text.append(nycphil['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1369/1369 [00:00<00:00, 3355.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(text))):\n",
    "    l=len(text[i])\n",
    "    if(l%2==1):\n",
    "        del text[i][l-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "merged = list(itertools.chain(*text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.Series(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[]\n",
    "response=[]\n",
    "for i in range(len(merged)):\n",
    "    if(i%2==0):\n",
    "        message.append(merged[i])\n",
    "    else:\n",
    "        response.append(merged[i])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(list(zip(message,response)), \n",
    "               columns =['message', 'response']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'd like to book a trip to Atlantis from Capri...</td>\n",
       "      <td>Hi...I checked a few options for you, and unfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes, how about going to Neverland from Caprica...</td>\n",
       "      <td>I checked the availability for this date and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have no flexibility for dates... but I can l...</td>\n",
       "      <td>I checked the availability for that date and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, I am looking to book a vacation from Go...</td>\n",
       "      <td>Hi. Sorry, I can't find any trips from Gotham ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What about a trip from Gotham City to Neverlan...</td>\n",
       "      <td>Sorry, I cannot find any trips leaving from Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Would any packages to Mos Eisley be available ...</td>\n",
       "      <td>There are no trips available to Mos Eisley.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You know what, I'd like to try and visit Never...</td>\n",
       "      <td>I cannot find any trips available to Neverland.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do you have any trips from Gotham City to Kobe...</td>\n",
       "      <td>I can book you a 3 day trip to Kobe leaving fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No, that's too far for me. I need a flight tha...</td>\n",
       "      <td>I can book you a trip from Birmingham to Kobe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many days would I be in Kobe?</td>\n",
       "      <td>You would arrive in Kobe August 17th and retur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  I'd like to book a trip to Atlantis from Capri...   \n",
       "1  Yes, how about going to Neverland from Caprica...   \n",
       "2  I have no flexibility for dates... but I can l...   \n",
       "3  Hello, I am looking to book a vacation from Go...   \n",
       "4  What about a trip from Gotham City to Neverlan...   \n",
       "5  Would any packages to Mos Eisley be available ...   \n",
       "6  You know what, I'd like to try and visit Never...   \n",
       "7  Do you have any trips from Gotham City to Kobe...   \n",
       "8  No, that's too far for me. I need a flight tha...   \n",
       "9                  How many days would I be in Kobe?   \n",
       "\n",
       "                                            response  \n",
       "0  Hi...I checked a few options for you, and unfo...  \n",
       "1  I checked the availability for this date and t...  \n",
       "2  I checked the availability for that date and t...  \n",
       "3  Hi. Sorry, I can't find any trips from Gotham ...  \n",
       "4  Sorry, I cannot find any trips leaving from Go...  \n",
       "5        There are no trips available to Mos Eisley.  \n",
       "6    I cannot find any trips available to Neverland.  \n",
       "7  I can book you a 3 day trip to Kobe leaving fr...  \n",
       "8  I can book you a trip from Birmingham to Kobe ...  \n",
       "9  You would arrive in Kobe August 17th and retur...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv('text.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pd.read_csv('../input/data-travel/text.csv')\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 9579/9579 [00:02<00:00, 3368.22it/s]\n",
      "100%|████████████████████████████████████| 9579/9579 [00:02<00:00, 3383.42it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "from tqdm import tqdm\n",
    "preprocessed_que = []\n",
    "preprocessed_ans = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(text['message'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    preprocessed_que.append(sentance.strip())\n",
    "\n",
    "for sentance in tqdm(text['response'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", str(sentance))\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    preprocessed_ans.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to book a trip to Atlantis from C...</td>\n",
       "      <td>Hi I checked a few options for you and unfortu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes how about going to Neverland from Caprica ...</td>\n",
       "      <td>I checked the availability for this date and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have no flexibility for dates but I can leav...</td>\n",
       "      <td>I checked the availability for that date and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello I am looking to book a vacation from Got...</td>\n",
       "      <td>Hi Sorry I can not find any trips from Gotham ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What about a trip from Gotham City to Neverlan...</td>\n",
       "      <td>Sorry I cannot find any trips leaving from Got...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  I would like to book a trip to Atlantis from C...   \n",
       "1  Yes how about going to Neverland from Caprica ...   \n",
       "2  I have no flexibility for dates but I can leav...   \n",
       "3  Hello I am looking to book a vacation from Got...   \n",
       "4  What about a trip from Gotham City to Neverlan...   \n",
       "\n",
       "                                            response  \n",
       "0  Hi I checked a few options for you and unfortu...  \n",
       "1  I checked the availability for this date and t...  \n",
       "2  I checked the availability for that date and t...  \n",
       "3  Hi Sorry I can not find any trips from Gotham ...  \n",
       "4  Sorry I cannot find any trips leaving from Got...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['message'] = preprocessed_que\n",
    "text['response'] = preprocessed_ans\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=text[0:8000]\n",
    "test=text[8000: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=train['message'].values\n",
    "test_input=test['message'].values\n",
    "\n",
    "train_target=train['response'].values\n",
    "test_target=test['response'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_len = 60\n",
    "\n",
    "embed_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=500)\n",
    "tokenizer.fit_on_texts(list(train_input) + list(test_input))\n",
    "\n",
    "train_q = tokenizer.texts_to_sequences(train_input)\n",
    "test_q = tokenizer.texts_to_sequences(test_input)\n",
    "\n",
    "x_train = pad_sequences(train_q, maxlen=max_input_len)\n",
    "x_test = pad_sequences(test_q, maxlen=max_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_len=60\n",
    "t = Tokenizer(num_words=500)\n",
    "t.fit_on_texts(list(train_target)+list(test_target))\n",
    "\n",
    "train_a = t.texts_to_sequences(train_target)\n",
    "test_a = t.texts_to_sequences(test_target)\n",
    "\n",
    "y_train = pad_sequences(train_a,maxlen=max_output_len)\n",
    "y_test = pad_sequences(test_a,maxlen=max_output_len)\n",
    "\n",
    "    \n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 60, 500) (8000, 60, 500) (8000, 60, 500)\n",
      "X1=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 19, 29, 2, 26, 6, 42, 2, 12, 27, 449, 48, 8, 74, 1, 15, 6, 51, 18], X2=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 7, 2, 267, 145, 8, 1, 5, 97, 32, 23, 49, 451, 9, 60, 102, 31, 20, 11, 1, 25, 4, 51, 72, 452, 93, 166], y=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 184, 7, 2, 267, 145, 8, 1, 5, 97, 32, 23, 49, 451, 9, 60, 102, 31, 20, 11, 1, 25, 4, 51, 72, 452, 93, 166]\n"
     ]
    }
   ],
   "source": [
    "X1, X2, y = list(), list(), list()\n",
    "\n",
    "target_in = y_train\n",
    "    \n",
    "src_encoded = to_categorical(x_train)\n",
    "tar_encoded = to_categorical(y_train)\n",
    "tar2_encoded = to_categorical(target_in)\n",
    "\n",
    "X1.append(src_encoded)\n",
    "X2.append(tar2_encoded)\n",
    "y.append(tar_encoded)\n",
    "\n",
    "X1=array(X1).reshape(8000,60,500)\n",
    "X2=array(X2).reshape(8000,60,500)\n",
    "y=array(y).reshape(8000,60,500)\n",
    "\n",
    "print(X1.shape, X2.shape, y.shape)\n",
    "print('X1=%s, X2=%s, y=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 500)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 500)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 322048      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 128),  322048      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 500)    64500       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 708,596\n",
      "Trainable params: 708,596\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_models(n_input, n_output, n_units):\n",
    "# define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "# define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "# return all models\n",
    "    return model, encoder_model, decoder_model\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "    # encode\n",
    "    state = infenc.predict(source)\n",
    "    # start of sequence input\n",
    "    target_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for t in range(n_steps):\n",
    "    # predict next char\n",
    "        yhat, h, c = infdec.predict([target_seq] + state)\n",
    "    # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "    # update state\n",
    "        state = [h, c]\n",
    "    # update target sequence\n",
    "        target_seq = yhat\n",
    "    return array(output)\n",
    " \n",
    "# configure problem\n",
    "n_features = 500\n",
    "n_steps_in = 60\n",
    "n_steps_out = 60\n",
    "# define model\n",
    "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5280/8000 [==================>...........] - ETA: 41:23 - loss: 6.1968 - acc: 0.0000e+ - ETA: 25:07 - loss: 6.1779 - acc: 0.0044   - ETA: 19:33 - loss: 6.1568 - acc: 0.27 - ETA: 16:54 - loss: 6.1366 - acc: 0.39 - ETA: 15:36 - loss: 6.1141 - acc: 0.47 - ETA: 14:26 - loss: 6.0906 - acc: 0.52 - ETA: 13:43 - loss: 6.0616 - acc: 0.57 - ETA: 13:06 - loss: 6.0289 - acc: 0.59 - ETA: 12:30 - loss: 5.9919 - acc: 0.61 - ETA: 11:57 - loss: 5.9405 - acc: 0.62 - ETA: 11:38 - loss: 5.8619 - acc: 0.64 - ETA: 11:21 - loss: 5.7565 - acc: 0.65 - ETA: 11:01 - loss: 5.6372 - acc: 0.66 - ETA: 10:43 - loss: 5.4940 - acc: 0.67 - ETA: 10:28 - loss: 5.3666 - acc: 0.68 - ETA: 10:23 - loss: 5.2265 - acc: 0.68 - ETA: 10:10 - loss: 5.0883 - acc: 0.69 - ETA: 9:57 - loss: 4.9643 - acc: 0.7015 - ETA: 10:36 - loss: 4.8197 - acc: 0.70 - ETA: 10:28 - loss: 4.6891 - acc: 0.71 - ETA: 10:18 - loss: 4.5758 - acc: 0.71 - ETA: 13:02 - loss: 4.4574 - acc: 0.72 - ETA: 12:43 - loss: 4.3364 - acc: 0.72 - ETA: 12:28 - loss: 4.2282 - acc: 0.72 - ETA: 12:16 - loss: 4.1249 - acc: 0.72 - ETA: 12:01 - loss: 4.0318 - acc: 0.73 - ETA: 11:45 - loss: 3.9430 - acc: 0.73 - ETA: 11:32 - loss: 3.8537 - acc: 0.73 - ETA: 11:32 - loss: 3.7738 - acc: 0.73 - ETA: 11:20 - loss: 3.6979 - acc: 0.74 - ETA: 11:08 - loss: 3.6449 - acc: 0.74 - ETA: 10:59 - loss: 3.5897 - acc: 0.74 - ETA: 10:49 - loss: 3.5238 - acc: 0.74 - ETA: 10:44 - loss: 3.4819 - acc: 0.74 - ETA: 10:34 - loss: 3.4312 - acc: 0.74 - ETA: 10:25 - loss: 3.3719 - acc: 0.74 - ETA: 10:17 - loss: 3.3234 - acc: 0.75 - ETA: 10:11 - loss: 3.2863 - acc: 0.75 - ETA: 10:04 - loss: 3.2481 - acc: 0.75 - ETA: 9:58 - loss: 3.2015 - acc: 0.7526 - ETA: 9:50 - loss: 3.1706 - acc: 0.752 - ETA: 9:43 - loss: 3.1330 - acc: 0.753 - ETA: 9:38 - loss: 3.0918 - acc: 0.754 - ETA: 9:30 - loss: 3.0596 - acc: 0.755 - ETA: 9:24 - loss: 3.0330 - acc: 0.755 - ETA: 9:17 - loss: 2.9979 - acc: 0.756 - ETA: 9:12 - loss: 2.9777 - acc: 0.755 - ETA: 9:17 - loss: 2.9496 - acc: 0.755 - ETA: 9:11 - loss: 2.9178 - acc: 0.757 - ETA: 9:05 - loss: 2.8884 - acc: 0.758 - ETA: 8:59 - loss: 2.8640 - acc: 0.758 - ETA: 8:54 - loss: 2.8348 - acc: 0.760 - ETA: 8:49 - loss: 2.8146 - acc: 0.760 - ETA: 8:45 - loss: 2.7948 - acc: 0.759 - ETA: 8:40 - loss: 2.7757 - acc: 0.759 - ETA: 8:35 - loss: 2.7548 - acc: 0.760 - ETA: 8:31 - loss: 2.7342 - acc: 0.760 - ETA: 8:27 - loss: 2.7119 - acc: 0.761 - ETA: 8:24 - loss: 2.6911 - acc: 0.761 - ETA: 8:20 - loss: 2.6728 - acc: 0.762 - ETA: 8:15 - loss: 2.6522 - acc: 0.762 - ETA: 8:10 - loss: 2.6332 - acc: 0.763 - ETA: 8:06 - loss: 2.6197 - acc: 0.763 - ETA: 8:03 - loss: 2.6022 - acc: 0.763 - ETA: 7:59 - loss: 2.5869 - acc: 0.763 - ETA: 7:55 - loss: 2.5713 - acc: 0.763 - ETA: 7:53 - loss: 2.5563 - acc: 0.764 - ETA: 7:49 - loss: 2.5417 - acc: 0.764 - ETA: 7:44 - loss: 2.5299 - acc: 0.764 - ETA: 7:39 - loss: 2.5185 - acc: 0.763 - ETA: 7:35 - loss: 2.5074 - acc: 0.763 - ETA: 7:32 - loss: 2.4930 - acc: 0.763 - ETA: 7:28 - loss: 2.4809 - acc: 0.764 - ETA: 7:24 - loss: 2.4675 - acc: 0.764 - ETA: 7:20 - loss: 2.4518 - acc: 0.764 - ETA: 7:17 - loss: 2.4407 - acc: 0.764 - ETA: 7:14 - loss: 2.4240 - acc: 0.765 - ETA: 7:10 - loss: 2.4136 - acc: 0.765 - ETA: 7:07 - loss: 2.4015 - acc: 0.766 - ETA: 7:03 - loss: 2.3923 - acc: 0.765 - ETA: 6:59 - loss: 2.3814 - acc: 0.765 - ETA: 6:57 - loss: 2.3680 - acc: 0.766 - ETA: 6:53 - loss: 2.3557 - acc: 0.766 - ETA: 6:49 - loss: 2.3477 - acc: 0.766 - ETA: 6:46 - loss: 2.3351 - acc: 0.767 - ETA: 6:42 - loss: 2.3241 - acc: 0.767 - ETA: 6:39 - loss: 2.3155 - acc: 0.767 - ETA: 6:35 - loss: 2.3023 - acc: 0.768 - ETA: 6:32 - loss: 2.2909 - acc: 0.768 - ETA: 6:28 - loss: 2.2806 - acc: 0.768 - ETA: 6:25 - loss: 2.2709 - acc: 0.768 - ETA: 6:22 - loss: 2.2635 - acc: 0.768 - ETA: 6:19 - loss: 2.2553 - acc: 0.768 - ETA: 6:16 - loss: 2.2457 - acc: 0.768 - ETA: 6:13 - loss: 2.2380 - acc: 0.768 - ETA: 6:11 - loss: 2.2310 - acc: 0.768 - ETA: 6:08 - loss: 2.2220 - acc: 0.768 - ETA: 6:05 - loss: 2.2160 - acc: 0.768 - ETA: 6:03 - loss: 2.2100 - acc: 0.768 - ETA: 6:00 - loss: 2.2018 - acc: 0.768 - ETA: 5:57 - loss: 2.1932 - acc: 0.768 - ETA: 5:54 - loss: 2.1836 - acc: 0.769 - ETA: 5:51 - loss: 2.1733 - acc: 0.769 - ETA: 5:51 - loss: 2.1668 - acc: 0.769 - ETA: 5:48 - loss: 2.1624 - acc: 0.769 - ETA: 5:50 - loss: 2.1546 - acc: 0.769 - ETA: 5:48 - loss: 2.1495 - acc: 0.769 - ETA: 5:45 - loss: 2.1440 - acc: 0.769 - ETA: 5:42 - loss: 2.1404 - acc: 0.768 - ETA: 5:39 - loss: 2.1354 - acc: 0.768 - ETA: 5:36 - loss: 2.1286 - acc: 0.768 - ETA: 5:39 - loss: 2.1224 - acc: 0.768 - ETA: 5:37 - loss: 2.1174 - acc: 0.768 - ETA: 5:35 - loss: 2.1097 - acc: 0.768 - ETA: 5:31 - loss: 2.1029 - acc: 0.768 - ETA: 5:31 - loss: 2.0963 - acc: 0.769 - ETA: 5:28 - loss: 2.0899 - acc: 0.769 - ETA: 5:26 - loss: 2.0852 - acc: 0.769 - ETA: 5:22 - loss: 2.0791 - acc: 0.769 - ETA: 5:20 - loss: 2.0743 - acc: 0.769 - ETA: 5:17 - loss: 2.0707 - acc: 0.768 - ETA: 5:14 - loss: 2.0648 - acc: 0.768 - ETA: 5:12 - loss: 2.0614 - acc: 0.768 - ETA: 5:09 - loss: 2.0562 - acc: 0.768 - ETA: 5:06 - loss: 2.0496 - acc: 0.768 - ETA: 5:04 - loss: 2.0448 - acc: 0.768 - ETA: 5:02 - loss: 2.0374 - acc: 0.769 - ETA: 4:59 - loss: 2.0319 - acc: 0.769 - ETA: 5:00 - loss: 2.0250 - acc: 0.769 - ETA: 4:57 - loss: 2.0195 - acc: 0.769 - ETA: 4:54 - loss: 2.0154 - acc: 0.769 - ETA: 4:51 - loss: 2.0095 - acc: 0.769 - ETA: 4:49 - loss: 2.0042 - acc: 0.769 - ETA: 4:47 - loss: 1.9973 - acc: 0.770 - ETA: 4:44 - loss: 1.9920 - acc: 0.770 - ETA: 4:42 - loss: 1.9865 - acc: 0.770 - ETA: 4:39 - loss: 1.9801 - acc: 0.771 - ETA: 4:41 - loss: 1.9757 - acc: 0.771 - ETA: 4:39 - loss: 1.9712 - acc: 0.771 - ETA: 4:37 - loss: 1.9683 - acc: 0.770 - ETA: 4:34 - loss: 1.9619 - acc: 0.771 - ETA: 4:31 - loss: 1.9581 - acc: 0.771 - ETA: 4:29 - loss: 1.9526 - acc: 0.771 - ETA: 4:27 - loss: 1.9491 - acc: 0.771 - ETA: 4:24 - loss: 1.9445 - acc: 0.771 - ETA: 4:21 - loss: 1.9430 - acc: 0.770 - ETA: 4:18 - loss: 1.9370 - acc: 0.771 - ETA: 4:16 - loss: 1.9319 - acc: 0.771 - ETA: 4:13 - loss: 1.9304 - acc: 0.770 - ETA: 4:10 - loss: 1.9270 - acc: 0.770 - ETA: 4:08 - loss: 1.9229 - acc: 0.770 - ETA: 4:05 - loss: 1.9183 - acc: 0.770 - ETA: 4:02 - loss: 1.9137 - acc: 0.771 - ETA: 4:00 - loss: 1.9102 - acc: 0.771 - ETA: 3:58 - loss: 1.9060 - acc: 0.771 - ETA: 3:55 - loss: 1.9027 - acc: 0.771 - ETA: 3:52 - loss: 1.9011 - acc: 0.770 - ETA: 3:50 - loss: 1.8974 - acc: 0.770 - ETA: 3:47 - loss: 1.8937 - acc: 0.770 - ETA: 3:45 - loss: 1.8895 - acc: 0.770 - ETA: 3:43 - loss: 1.8856 - acc: 0.770 - ETA: 3:40 - loss: 1.8822 - acc: 0.770 - ETA: 3:38 - loss: 1.8777 - acc: 0.771 - ETA: 3:35 - loss: 1.8733 - acc: 0.771 - ETA: 3:33 - loss: 1.8707 - acc: 0.7710"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fefba7cd3fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1202\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m                             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=train.fit([X1, X2], y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, X_2, Y = list(), list(), list()\n",
    "\n",
    "target_in = y_test\n",
    "    \n",
    "src_encoded = to_categorical(x_test,num_classes=500)\n",
    "tar_encoded = to_categorical(y_test,num_classes=500)\n",
    "tar2_encoded = to_categorical(target_in,num_classes=500)\n",
    "\n",
    "X_1.append(src_encoded)\n",
    "X_2.append(tar2_encoded)\n",
    "Y.append(tar_encoded)\n",
    "\n",
    "X_1=array(X_1).reshape(1579,60,500)\n",
    "X_2=array(X_2).reshape(1579,60,500)\n",
    "Y=array(Y).reshape(1579,60,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=X_1[0:1:,]\n",
    "x2=X_1[1:2:,]\n",
    "x3=X_1[2:3:,]\n",
    "x4=X_1[3:4:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = predict_sequence(infenc, infdec, x4, n_steps_out, n_features)\n",
    "print('X=%s y=%s, yhat=%s' % (one_hot_decode(X_1[3]), one_hot_decode(Y[3]),one_hot_decode(target)))\n",
    "y_pred=one_hot_decode(target)\n",
    "a=t.word_index\n",
    "b=list(a.keys())\n",
    "reply=[]\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if y_pred[j]==i+1:\n",
    "            reply.append(b[i])\n",
    "\n",
    "print(\"*****************EMAIL_BODY********************\\n\")\n",
    "\n",
    "print(test_input[3])\n",
    "\n",
    "print(\"*******************Suggested Reply**************\")\n",
    "\n",
    "print(' '.join(reply))\n",
    "\n",
    "print(\"*******************Actual Reply******************\")\n",
    "print(test_target[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
